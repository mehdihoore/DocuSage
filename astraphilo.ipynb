!pip install astrapy langchain-community openai pypdf python-dotenv docx2txt tiktoken nltk sentence-transformers transformers torch hazm
import os
import json
import uuid
from pathlib import Path
from typing import Any, List, Tuple, Optional
from dataclasses import dataclass
from pypdf import PdfReader
from langchain.docstore.document import Document
from langchain.vectorstores import VectorStore
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import Docx2txtLoader
from langchain.document_loaders import PyPDFLoader
from langchain.document_loaders import TextLoader
from astrapy import DataAPIClient
import re
from google.colab import userdata
from astrapy.constants import VectorMetric
from astrapy.database import Database
from astrapy.collection import Collection
from astrapy.exceptions import *
import time
import google.generativeai as genai
import requests
import pandas as pd
import csv


# Constants and configurations
TEXT_FILE_TYPES = ["txt", "docx", "pdf"]
ASTRA_DB_APPLICATION_TOKEN = userdata.get("PHILO_API")
ASTRA_DB_API_ENDPOINT = userdata.get("PHILO_END")
GEMINI_API_KEY = userdata.get("GOOGLE_API_KEY_BHR1").strip()
GEMINI_EMBEDDING_MODEL_NAME = "models/text-embedding-004"
GEMINI_TEXT_MODEL_NAME = "gemini-1.5-flash-8b"

# Cloudflare Workers AI configuration
CLOUDFLARE_API_BASE_URL = "https://api.cloudflare.com/client/v4/accounts/67f16cc2dc0c1850198f5fd061d3cdf2/ai/run/"
CLOUDFLARE_API_TOKEN = "uSjuZf1K9IydTB-1aHfoHkqGApCyjHJht-mp-sA6" # Make sure to replace this with your actual token

CLOUDFLARE_MODEL = "@cf/meta/llama-3-8b-instruct"

# CSV file configuration
CSV_FILE_PATH = "/content/drive/MyDrive/processed_data.csv"
CSV_COLUMNS = ["doc_id", "doc_name", "chunk_number", "content", "embedding", "keywords", "summary", "references"]

# --- Error Handling ---
class EmbeddingError(Exception):
    pass

class AstraDBError(Exception):
    pass

class FileProcessingError(Exception):
    pass

class SummarizationError(Exception):
    pass
class KeywordExtractionError(Exception):
    pass

class CSVError(Exception):
    pass

# --- PDF Loader (using PyPDFLoader from LangChain for simplicity) ---
class PDFLoader:
    def __init__(self, file_path):
        self.file_path = file_path
        self.loader = PyPDFLoader(file_path)

    def load(self) -> List[Document]:
        try:
            return self.loader.load_and_split()
        except Exception as e:
            raise FileProcessingError(f"Error loading PDF: {e}")

@dataclass
class Data:
    content: str = ""
    references: str = ""

# --- Document Processor ---
class DocumentProcessor:
    def __init__(self, path: str, silent_errors: bool = False):
        self.path = path
        self.silent_errors = silent_errors
        self.status = ""

    def resolve_path(self, path: str) -> str:
        return os.path.abspath(path)

    def load_file(self) -> Tuple[Data, str]:
        if not self.path:
            raise ValueError("Please upload a file to use this component.")

        resolved_path = self.resolve_path(self.path)
        extension = Path(resolved_path).suffix[1:].lower()

        if extension not in TEXT_FILE_TYPES:
            raise ValueError(f"Unsupported file type: {extension}")

        if extension == "docx":
            loader = Docx2txtLoader(resolved_path)
        elif extension == "pdf":
            loader = PDFLoader(resolved_path)
        else:
            loader = TextLoader(resolved_path)

        try:
            data_list = loader.load()
        except Exception as e:
            raise FileProcessingError(f"Error loading file: {e}")

        if isinstance(data_list, list) and len(data_list) > 0:
            combined_text = " ".join([doc.page_content for doc in data_list])
            data = Data(content=combined_text)
            return data, Path(resolved_path).stem
        else:
            raise FileProcessingError("No data loaded from file")

# --- Text Splitter ---
class RecursiveCharacterTextSplitterComponent:
    def __init__(self):
        self.splitter = RecursiveCharacterTextSplitter(
            separators=["\n\n", "\n", ". ", " "],
            chunk_size=5000,
            chunk_overlap=500
        )

    def split_text(self, text: str) -> List[str]:
        return self.splitter.split_text(text)

# --- Gemini Embeddings ---
class GeminiEmbeddingsComponent:
    def __init__(self, gemini_api_key: str, max_retries: int = 5, initial_retry_delay: int = 60, backoff_factor: float = 2.0):
        genai.configure(api_key=gemini_api_key)
        self.embedding_model = GEMINI_EMBEDDING_MODEL_NAME
        self.text_model = GEMINI_TEXT_MODEL_NAME
        self.max_retries = max_retries
        self.retry_delay = initial_retry_delay
        self.backoff_factor = backoff_factor
        self.cloudflare_models = [
            {"name": "@cf/meta/llama-3.2-1b-instruct", "endpoint": CLOUDFLARE_API_BASE_URL},
            {"name": "@cf/meta/llama-3.3-70b-instruct-fp8-fast", "endpoint": CLOUDFLARE_API_BASE_URL},
            {"name": "@cf/meta/llama-3.1-8b-instruct-fast", "endpoint": CLOUDFLARE_API_BASE_URL},
            {"name": "@cf/google/gemma-7b-it-lora", "endpoint": CLOUDFLARE_API_BASE_URL},
            {"name": "@cf/google/gemma-13b-it-lora", "endpoint": CLOUDFLARE_API_BASE_URL}
        ]
        self.current_model_index = 0
    def _rotate_cloudflare_model(self):
        """Rotate to the next Cloudflare AI model."""
        self.current_model_index = (self.current_model_index + 1) % len(self.cloudflare_models)
        return self.cloudflare_models[self.current_model_index]
    def _call_cloudflare_ai(self, prompt: str, task: str = "keywords") -> str:
        """
        Call Cloudflare AI with a specific model and prompt.

        :param prompt: The input prompt for the AI
        :param task: Either "keywords" or "summary"
        :return: AI-generated response
        """
        # Select the current model
        current_model = self.cloudflare_models[self.current_model_index]

        headers = {"Authorization": f"Bearer {CLOUDFLARE_API_TOKEN}"}

        # Prepare system and user messages based on the task
        if task == "keywords":
            system_message = "You are a helpful assistant that extracts keywords from text."
            user_message = f"Extract 5-7 most significant keywords from the text:\n{prompt[:2000]}"
        else:  # summary
            system_message = "You are a helpful assistant that summarizes text."
            user_message = f"Provide a concise summary of the text:\n{prompt[:2000]}"

        inputs = [
            {"role": "system", "content": system_message},
            {"role": "user", "content": user_message}
        ]

        input_data = {"messages": inputs}

        try:
            response = requests.post(
                f"{current_model['endpoint']}{current_model['name']}",
                headers=headers,
                json=input_data
            )
            response.raise_for_status()
            result = response.json()

            if result and result.get("result") and result["result"].get("response"):
                return result["result"]["response"].strip()
            else:
                # If no valid response, rotate to next model and retry
                print(f"No response from {current_model['name']}. Rotating models.")
                self._rotate_cloudflare_model()
                raise Exception("No valid response from current model")

        except requests.exceptions.RequestException as e:
            print(f"Error calling {current_model['name']}: {e}")
            # Rotate to next model and re-raise the exception
            self._rotate_cloudflare_model()
            raise

    def _retry_on_exception(self, func, *args, **kwargs):
        """Helper function for retries with exponential backoff."""
        retries = 0
        current_delay = self.retry_delay

        while retries < self.max_retries:
            try:
                return func(*args, **kwargs)
            except Exception as e:
                print(f"Attempt {retries + 1} failed: {e}")
                retries += 1  # Increment retry counter

                # Check if the error is related to rate limiting or other retryable issues
                if 'quota' in str(e).lower() or isinstance(e, genai.types.generation_types.BlockedPromptException):
                    print(f"Waiting {current_delay} seconds before retry...")
                    time.sleep(current_delay)
                    current_delay *= self.backoff_factor  # Exponential backoff
                else:
                    print("Non-retryable error encountered. Skipping retries.")
                    break  # Exit retry loop for non-retryable errors
        # If max retries are reached or a non-retryable error occurred, raise the exception
        raise EmbeddingError("Max retries reached for embedding generation or non-retryable error occurred")

    def extract_keywords(self, text: str) -> List[str]:
        """Extract keywords with model rotation."""
        try:
           # Try Cloudflare AI models with rotation
            for _ in range(len(self.cloudflare_models)):
                try:
                    result = self._call_cloudflare_ai(text, task="keywords")
                    keywords = [k.strip() for k in result.split(',') if k.strip()]
                    return keywords
                except Exception as model_error:
                    print(f"Model rotation error: {model_error}")

            # First try Gemini

        except Exception as e:
            print(f"Gemini keyword extraction failed: {e}. Falling back to Cloudflare Workers AI.")
            return self._extract_keywords_gemini(text)


            # If all models fail
            raise KeywordExtractionError("All models failed to extract keywords")

    """def extract_keywords(self, text: str) -> List[str]:
        #Extract keywords with retry mechanism.
        def _extract_keywords(text):
            model = genai.GenerativeModel(self.text_model)
            prompt = f"Extract 5-7 most significant keywords from the text:
            {text[:2000]}"

            response = model.generate_content(prompt)
            keywords = [k.strip() for k in response.text.split(',') if k.strip()]
            return keywords

        return self._retry_on_exception(_extract_keywords, text)"""
    def _extract_keywords_gemini(self, text: str) -> List[str]:
        """Helper function to extract keywords using the Gemini model."""
        model = genai.GenerativeModel(self.text_model)
        prompt = f"""Extract 5-7 most significant keywords from the text:
        {text[:2000]}"""

        try:
            response = model.generate_content(prompt)
            keywords = [k.strip() for k in response.text.split(',') if k.strip()]
            return keywords
        except Exception as e:
            raise KeywordExtractionError(f"Gemini keyword extraction failed: {e}")

    def _extract_keywords_cloudflare(self, text: str) -> List[str]:
        """Helper function to extract keywords using Cloudflare Workers AI."""
        headers = {"Authorization": f"Bearer {CLOUDFLARE_API_TOKEN}"}
        inputs = [
            {"role": "system", "content": "You are a helpful assistant that extracts keywords from text."},
            {"role": "user", "content": f"Extract 5-7 most significant keywords from the text:\n{text[:2000]}"}
        ]
        input_data = {"messages": inputs}
        try:
            response = requests.post(
                f"{CLOUDFLARE_API_BASE_URL}{CLOUDFLARE_MODEL}",
                headers=headers,
                json=input_data
            )
            response.raise_for_status()
            result = response.json()

            if result and result.get("result") and result["result"].get("response"):
                # Assuming the response is a comma-separated list of keywords
                keywords = [k.strip() for k in result["result"]["response"].split(',') if k.strip()]
                return keywords
            else:
                raise KeywordExtractionError(f"Unexpected response from Cloudflare Workers AI: {result}")

        except requests.exceptions.RequestException as e:
            raise KeywordExtractionError(f"Error calling Cloudflare Workers AI: {e}")


    def generate_chunk_summary(self, text: str) -> str:
        """Generate summary with model rotation."""
        try:
            for _ in range(len(self.cloudflare_models)):
                  try:
                      return self._call_cloudflare_ai(text, task="summary")
                  except Exception as model_error:
                      print(f"Model rotation error: {model_error}")
            # First try Gemini

        except Exception as e:
            print(f"Gemini summarization failed: {e}. Falling back to Cloudflare Workers AI.")
            return self._generate_gemini_summary(text)

            # Try Cloudflare AI models with rotation


            # If all models fail
            raise SummarizationError("All models failed to generate summary")

    def _generate_gemini_summary(self, text: str) -> str:
        """Helper function to generate summaries using the Gemini model."""
        model = genai.GenerativeModel(self.text_model)
        prompt = f"""Provide a concise summary of the text:
        {text[:2000]}"""
        try:
            response = model.generate_content(prompt)
            return response.text.strip()
        except Exception as e:
            raise SummarizationError(f"Gemini summarization failed: {e}")

    def _generate_cloudflare_summary(self, text: str) -> str:
        """Helper function to generate summaries using Cloudflare Workers AI."""
        headers = {"Authorization": f"Bearer {CLOUDFLARE_API_TOKEN}"}
        inputs = [
            {"role": "system", "content": "You are a helpful assistant that summarizes text."},
            {"role": "user", "content": f"Provide a concise summary of the text:\n{text[:2000]}"}
        ]
        input_data = {"messages": inputs}
        try:
            response = requests.post(
                f"{CLOUDFLARE_API_BASE_URL}{CLOUDFLARE_MODEL}",
                headers=headers,
                json=input_data
            )
            response.raise_for_status()  # Raise an exception for bad status codes
            result = response.json()

            if result and result.get("result") and result["result"].get("response"):
                return result["result"]["response"].strip()
            else:
                raise SummarizationError(f"Unexpected response from Cloudflare Workers AI: {result}")

        except requests.exceptions.RequestException as e:
            raise SummarizationError(f"Error calling Cloudflare Workers AI: {e}")

    def build_embeddings(self, texts: List[str], expected_dim: int = 768) -> List[List[float]]:
        embeddings = []
        batch_size = 5

        if not texts:
            print("No texts provided for embedding")
            return []

        if len(texts) > 100:
            print(f"Large number of texts detected. Processing in chunks of 100.")

        for i in range(0, len(texts), 100):
            batch_texts = texts[i:i + 100]

            for j in range(0, len(batch_texts), batch_size):
                batch = batch_texts[j:j + batch_size]

                try:
                    result = self._retry_on_exception(
                        genai.embed_content,
                        model=self.embedding_model,
                        content=batch,
                        task_type="retrieval_document"
                    )

                    if 'embedding' in result and result['embedding']:
                        for embedding in result['embedding']:
                            if len(embedding) == expected_dim:
                                embeddings.append(embedding)
                            else:
                                print(f"Warning: Skipping embedding with incorrect dimension (expected {expected_dim}, got {len(embedding)})")
                    else:
                        print("No embeddings returned for this batch")

                except EmbeddingError as e:
                    print(f"Failed to generate embeddings for batch: {e}")

            if i + 100 < len(texts):
                print("Pausing before processing the next chunk of 100 texts...")
                time.sleep(self.retry_delay)

        return embeddings

# --- Astra DB Manager ---
class AstraDBManager:
    def __init__(self, api_endpoint: str, token: str, collection_name: str, namespace: Optional[str] = None):
        self.api_endpoint = api_endpoint
        self.token = token
        self.collection_name = collection_name
        self.namespace = namespace or "default_namespace"

        try:
            self.client = DataAPIClient(token)
            self.database = self.client.get_database(api_endpoint)
        except (UnauthorizedException, DataAPIException, AstraPyException) as e:
            raise AstraDBError(f"Failed to initialize Astra DB client or database: {e}")



    def get_or_create_collection(self, collection_name: str, dimension: int = 768):
        try:
            print(f"Checking for collection {collection_name} in database.")
            collections = self.database.list_collections()
            if collection_name in collections:
                print(f"* Collection {collection_name} already exists.")
                return self.database.get_collection(collection_name)
            else:
                print(f"* Collection {collection_name} does not exist. Creating...")
                collection = self.database.create_collection(
                    name=collection_name,
                    dimension=dimension,
                    metric=VectorMetric.COSINE,
                )
                print(f"* Collection {collection_name} created successfully.")
                return collection
        except CollectionAlreadyExistsException:
            print(f"* Collection {collection_name} already exists. Skipping creation.")
            return self.database.get_collection(collection_name)
        except Exception as e:
            print(f"Error handling collection {collection_name}: {e}")
            raise

    def add_documents(self, embeddings: List[List[float]], doc_name: str, references: str,
                      chunks: List[str], keywords_list: List[List[str]], summaries: List[str]):
        if not isinstance(embeddings, list) or (embeddings and not isinstance(embeddings[0], list)):
            raise ValueError("Embeddings must be a list of lists")

        collection = self.get_or_create_collection(collection_name=self.collection_name)
        insertion_errors = 0
        successful_insertions = 0
        min_length = min(len(embeddings), len(chunks), len(keywords_list), len(summaries))
        batch_data = []
        for index in range(min_length):
            try:
                if len(embeddings[index]) != 768:
                    print(f"Warning: Skipping document {index} - incorrect embedding dimension")
                    continue

                content_chunk = chunks[index][:15000]

                doc_id = str(uuid.uuid4())
                document = {
                    "_id": doc_id,
                    "content": content_chunk,
                    "$vector": embeddings[index],
                    "metadata": {
                        "doc_name": doc_name,
                        "references": references,
                        "keywords": keywords_list[index],
                        "summary": summaries[index],
                        "chunk_number": index
                    }
                }

                result = collection.insert_one(document)

                if result and result.inserted_id:
                    successful_insertions += 1
                    print(f"Successfully inserted document {doc_id}")
                else:
                    insertion_errors += 1
                    print(f"Failed to insert document {doc_id}: No error message returned from Astra DB")

            except (DataAPIException, AstraPyException) as e:
                insertion_errors += 1
                print(f"Insertion error for document {index}: {e}")
            # Save the data to CSV as a backup
            try:
                self.save_to_csv(batch_data)
                print("Data saved to CSV as backup.")
            except CSVError as e:
                print(f"Failed to save data to CSV: {e}")

        print(f"Insertion Summary:")
        print(f"Total documents: {min_length}")
        print(f"Successful insertions: {successful_insertions}")
        print(f"Failed insertions: {insertion_errors}")
    def save_to_csv(self, data: List[dict]):
        """Saves the processed data to a CSV file."""
        try:
            df = pd.DataFrame(data)
            # Reorder columns to match CSV_COLUMNS
            df = df.reindex(columns=CSV_COLUMNS)

            # Check if the CSV file exists and has content
            file_exists = os.path.isfile(CSV_FILE_PATH) and os.path.getsize(CSV_FILE_PATH) > 0

            # If it exists and has content, append without header
            if file_exists:
                df.to_csv(CSV_FILE_PATH, mode='a', header=False, index=False)
            else:
                # Otherwise, create a new file with header
                df.to_csv(CSV_FILE_PATH, mode='w', header=True, index=False)

        except Exception as e:
            raise CSVError(f"Error saving data to CSV: {e}")

# --- Helper Functions ---
def extract_references(text: str) -> str:
    pattern = r'\d+-\d+-\d+-\d+'
    matches = re.findall(pattern, text)
    return matches[0] if matches else "No references found"

# --- Main Function ---
def main(file_paths: List[str]):
    text_splitter = RecursiveCharacterTextSplitterComponent()
    embeddings_component = GeminiEmbeddingsComponent(gemini_api_key=GEMINI_API_KEY)
    astradb_manager = AstraDBManager(
        api_endpoint=ASTRA_DB_API_ENDPOINT,
        token=ASTRA_DB_APPLICATION_TOKEN,
        collection_name="philosophical_texts"
    )

    for file_path in file_paths:
        print(f"Processing file: {file_path}")
        try:
            processor = DocumentProcessor(file_path)
            data, doc_name = processor.load_file()

            if not data.content:
                raise FileProcessingError(f"No content extracted from {file_path}")

            chunks = text_splitter.split_text(data.content)
            print(f"Total chunks extracted: {len(chunks)}")

            if not chunks:
                raise FileProcessingError("No chunks extracted from the document")

            embeddings = embeddings_component.build_embeddings(chunks)

            if not embeddings:
                raise EmbeddingError("No embeddings could be generated")

            keywords_list = [embeddings_component.extract_keywords(chunk) for chunk in chunks]
            summaries = [embeddings_component.generate_chunk_summary(chunk) for chunk in chunks]
            references = extract_references(data.content)
            # Add the data to Astra DB and also save to CSV as backup
            astradb_manager.add_documents(embeddings, doc_name, references, chunks, keywords_list, summaries)
            print(f"Finished processing file: {file_path}")

        except (FileProcessingError, EmbeddingError, AstraDBError, SummarizationError, CSVError) as e:
            print(f"Error processing {file_path}: {e}")

if __name__ == "__main__":
    file_paths = [

        "/content/drive/MyDrive/docfolder/(Philosophische Bibliothek_ 171b-d) Hegel, Georg Wilhelm Friedrich - Vorlesungen über die Philosophie der Weltgeschichte. Band II–IV_ Die orientalische Welt. Die griechische und die römische Welt. Die.pdf"]
    main(file_paths)
